<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GPU on Weblog</title><link>https://ronanquigley.com/blog/tags/gpu/</link><description>Recent content in GPU on Weblog</description><generator>Hugo -- gohugo.io</generator><language>en-uk</language><lastBuildDate>Tue, 11 Jun 2024 14:13:34 +0000</lastBuildDate><atom:link href="https://ronanquigley.com/blog/tags/gpu/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding GPU sharing strategies in Kubernetes</title><link>https://ronanquigley.com/blog/understanding-gpu-sharing-strategies-in-kubernetes/</link><pubDate>Tue, 11 Jun 2024 14:13:34 +0000</pubDate><guid>https://ronanquigley.com/blog/understanding-gpu-sharing-strategies-in-kubernetes/</guid><description>Understanding GPU sharing strategies in Kubernetes These notes are aimed at anyone that wants to setup Nvidia GPU sharing strategies within k8s without having to trawl through a lot of crypic and dense Nvidia documentation. I&amp;rsquo;m also focusing on a high level ELI5, using the knowledge I&amp;rsquo;ve gained so far on the subject, of:
MIG (Multi-Instance GPUs) MPS (Multi-process service) Time Slicing I used two Nvidia cards to put these notes together:</description></item></channel></rss>